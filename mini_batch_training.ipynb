{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.core.debugger import set_trace\n",
    "from fastai import datasets\n",
    "import pickle, gzip, math, torch, matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import tensor, nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'\n",
    "def get_data():\n",
    "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return map(tensor, (x_train,y_train,x_valid,y_valid))\n",
    "\n",
    "def normalize(x, m, s): return (x-m)/s\n",
    "\n",
    "def stats(x): return x.mean(),x.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST datasetup\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "x_train,y_train,x_valid,y_valid = get_data()\n",
    "\n",
    "n,m = x_train.shape\n",
    "c = y_train.max()+1\n",
    "number_hid = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module): #simple 3 layer Model\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "            super().__init__() #initalize nn.Module\n",
    "            self.layers = [nn.Linear(num_inputs, num_hidden),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Linear(num_hidden, num_outputs)\n",
    "                          ]\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, number_hid, 10)\n",
    "pred = model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$ \n",
    "\n",
    "\n",
    "$$\\hbox{Cross Entropy Loss}$$ $$-\\sum x\\, \\log p(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax_standard(x): \n",
    "    return (x.exp()/(x.exp().sum(-1, keepdim=True))).log()\n",
    "\n",
    "def log_softmax_simplified(x):\n",
    "    return x - x.exp().sum(-1, keepdim=True).log()\n",
    "\n",
    "#pytorch one-hot encoded Negative-Log Likelyhood\n",
    "#indexs by actuals, using numpy integer array indexing over the number of rows\n",
    "def one_hot_encoded_nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_pred = log_softmax_simplified(pred)\n",
    "\n",
    "loss = one_hot_encoded_nll(sm_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of model: 2.295798\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss of model: %f\"%(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hbox{Implementing the LogSumExp Trick}$$\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.295799\n",
      "Pytorch Loss: 2.295807\n"
     ]
    }
   ],
   "source": [
    "#this allows us to find our largest input, subtract it from the other inputs, and then add it back  in\n",
    "#this gives us the same result without worrying about e^(x) overflowing\n",
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0]\n",
    "    return m + (x-m[:, None]).exp().sum(-1).log()\n",
    "\n",
    "def log_softmax(x):\n",
    "    return x - x.logsumexp(-1, keepdim=True)\n",
    "\n",
    "#using softmax with nll\n",
    "loss = one_hot_encoded_nll(log_softmax(pred), y_train)\n",
    "pytorch_loss = F.nll_loss(F.log_softmax(pred, -1), y_train)\n",
    "print(\"Loss: %f\\nPytorch Loss: %f\"%(loss,pytorch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9688)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(out, yb):\n",
    "    return (torch.argmax(out, dim=1) == yb).float().mean()\n",
    "\n",
    "loss_function = F.cross_entropy\n",
    "batch_size = 64\n",
    "learning_rate = 0.5\n",
    "num_epochs = 1\n",
    "\n",
    "\n",
    "\n",
    "x_mini_batch = x_train[0:batch_size]\n",
    "preds = model(mini_batch)\n",
    "preds[0], preds.shape #64 batchsize * 10 categories\n",
    "\n",
    "y_mini_batch = y_train[0:batch_size]\n",
    "loss_function(preds, yb)\n",
    "accuracy(preds, yb) #terrible, which is expected for an untrained model randomly guessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "a basic training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0906, grad_fn=<NllLossBackward>), tensor(0.9844))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i in range((n-1)//batch_size +1):\n",
    "        start_index = i *batch_size\n",
    "        xmb = x_train[start_index: start_index + batch_size]\n",
    "        ymb = y_train[start_index: start_index + batch_size]\n",
    "        predictions = model(xmb)\n",
    "        loss = loss_function(predictions, ymb)\n",
    "        loss.backward() #calculate the gradients\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers: #only updating layers with parameters, ie: not ReLUs\n",
    "                if hasattr(l, 'weight'):\n",
    "                    l.weight -= l.weight.grad * learning_rate\n",
    "                    l.bias -= l.bias.grad * learning_rate\n",
    "                    #zero the matricies' gradients\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()\n",
    "loss_function(model(mini_batch), y_mini_batch), accuracy(model(x_mini_batch), y_mini_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Implementing Pytorch model.Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding capability to store list of attributes in our modules\n",
    "```\n",
    "class Module():\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "        self._modules = {} #empty set to store our modules in\n",
    "        self.layer1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.layer2 = nn.Linear(num_hidden, num_outputs) #store layers in a list, this is inflexible\n",
    "        \n",
    "    def __setattr__(self, k,v):\n",
    "        if not k.startswith(\"_\"):\n",
    "            self._modules[k] = v\n",
    "            super().__setattr__(k,v) #using python object's __setattr__\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self._modules}'\n",
    "\n",
    "    def parameters(self):\n",
    "        for l in self._modules.values():\n",
    "            for p in l.parameters():\n",
    "                yield p\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plain example\n",
    "class DumberModel(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.layer2 = nn.Linear(num_hidden, num_outputs)\n",
    "    def __call__(self, x):\n",
    "        return self.layer2(F.relu(self.layer1(x)))\n",
    "\n",
    "class DumbModel(nn.Module):\n",
    "    def __init__(self,layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for i, l in enumerate(self.layers): #add each layer as a Pytorch Module\n",
    "            self.add_module(f'layer_{i}', l) #name_index and the layer itself\n",
    "    def __call__(self, x):\n",
    "        return self.layer2(F.relu(self.layer1(x)))\n",
    "\n",
    "    \n",
    "#sequential model example\n",
    "class SequentialModel(nn.Module): #implementation of nn.Sequential\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range((n-1)//batch_size +1):\n",
    "            start_index = i * batch_size\n",
    "            x_mb = x_train[start_index:start_index + batch_size]\n",
    "            y_mb = y_train[start_index:start_index + batch_size]\n",
    "            predictions = model(x_mb)\n",
    "            loss = loss_function(predictions, y_mb)\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for parameter in model.parameters():\n",
    "                    parameter -= parameter.grad * learning_rate\n",
    "                model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Using the Sequential Layer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequentialModel(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_categories = 10\n",
    "layers = [nn.Linear(m, number_hid), nn.ReLU(), nn.Linear(number_hid, num_categories)]\n",
    "model = SequentialModel(layers)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1041, grad_fn=<NllLossBackward>), tensor(0.9531))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function = F.cross_entropy\n",
    "batch_size = 64\n",
    "learning_rate = 0.5\n",
    "num_epochs = 1\n",
    "fit()\n",
    "loss_function(model(x_mini_batch), y_mini_batch), accuracy(model(x_mini_batch), y_mini_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pytorch's nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1571, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(m, number_hid), nn.ReLU(), nn.Linear(number_hid, num_categories))\n",
    "fit()\n",
    "loss_function(model(x_mini_batch), y_mini_batch), accuracy(model(x_mini_batch), y_mini_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
