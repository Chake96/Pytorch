{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.core.debugger import set_trace\n",
    "from fastai import datasets\n",
    "import pickle, gzip, math, torch, matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import tensor, nn, optim\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'\n",
    "def get_data():\n",
    "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return map(tensor, (x_train,y_train,x_valid,y_valid))\n",
    "\n",
    "def normalize(x, m, s): return (x-m)/s\n",
    "\n",
    "def stats(x): return x.mean(),x.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST datasetup\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "x_train,y_train,x_valid,y_valid = get_data()\n",
    "\n",
    "n,m = x_train.shape\n",
    "c = y_train.max()+1\n",
    "number_hid = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module): #simple 3 layer Model\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "            super().__init__() #initalize nn.Module\n",
    "            self.layers = [nn.Linear(num_inputs, num_hidden),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Linear(num_hidden, num_outputs)\n",
    "                          ]\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, number_hid, 10)\n",
    "pred = model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$ \n",
    "\n",
    "\n",
    "$$\\hbox{Cross Entropy Loss}$$ $$-\\sum x\\, \\log p(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax_standard(x): \n",
    "    return (x.exp()/(x.exp().sum(-1, keepdim=True))).log()\n",
    "\n",
    "def log_softmax_simplified(x):\n",
    "    return x - x.exp().sum(-1, keepdim=True).log()\n",
    "\n",
    "#pytorch one-hot encoded Negative-Log Likelyhood\n",
    "#indexs by actuals, using numpy integer array indexing over the number of rows\n",
    "def one_hot_encoded_nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_pred = log_softmax_simplified(pred)\n",
    "\n",
    "loss = one_hot_encoded_nll(sm_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of model: 2.299490\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss of model: %f\"%(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hbox{Implementing the LogSumExp Trick}$$\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.299490\n",
      "Pytorch Loss: 2.299495\n"
     ]
    }
   ],
   "source": [
    "#this allows us to find our largest input, subtract it from the other inputs, and then add it back  in\n",
    "#this gives us the same result without worrying about e^(x) overflowing\n",
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0]\n",
    "    return m + (x-m[:, None]).exp().sum(-1).log()\n",
    "\n",
    "def log_softmax(x):\n",
    "    return x - x.logsumexp(-1, keepdim=True)\n",
    "\n",
    "#using softmax with nll\n",
    "loss = one_hot_encoded_nll(log_softmax(pred), y_train)\n",
    "pytorch_loss = F.nll_loss(F.log_softmax(pred, -1), y_train)\n",
    "print(\"Loss: %f\\nPytorch Loss: %f\"%(loss,pytorch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0938)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(out, yb):\n",
    "    return (torch.argmax(out, dim=1) == yb).float().mean()\n",
    "\n",
    "loss_function = F.cross_entropy\n",
    "batch_size = 64\n",
    "learning_rate = 0.5\n",
    "num_epochs = 1\n",
    "\n",
    "\n",
    "\n",
    "x_mini_batch = x_train[0:batch_size]\n",
    "preds = model(x_mini_batch)\n",
    "preds[0], preds.shape #64 batchsize * 10 categories\n",
    "\n",
    "y_mini_batch = y_train[0:batch_size]\n",
    "loss_function(preds, y_mini_batch)\n",
    "accuracy(preds, y_mini_batch) #terrible, which is expected for an untrained model randomly guessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "a basic training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0006, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i in range((n-1)//batch_size +1):\n",
    "        start_index = i *batch_size\n",
    "        xmb = x_train[start_index: start_index + batch_size]\n",
    "        ymb = y_train[start_index: start_index + batch_size]\n",
    "        predictions = model(x_mini_batch)\n",
    "        loss = loss_function(predictions, y_mini_batch)\n",
    "        loss.backward() #calculate the gradients\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers: #only updating layers with parameters, ie: not ReLUs\n",
    "                if hasattr(l, 'weight'):\n",
    "                    l.weight -= l.weight.grad * learning_rate\n",
    "                    l.bias -= l.bias.grad * learning_rate\n",
    "                    #zero the matricies' gradients\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()\n",
    "loss_function(model(x_mini_batch), y_mini_batch), accuracy(model(x_mini_batch), y_mini_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Implementing Pytorch model.Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding capability to store list of attributes in our modules\n",
    "```\n",
    "class Module():\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "        self._modules = {} #empty set to store our modules in\n",
    "        self.layer1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.layer2 = nn.Linear(num_hidden, num_outputs) #store layers in a list, this is inflexible\n",
    "        \n",
    "    def __setattr__(self, k,v):\n",
    "        if not k.startswith(\"_\"):\n",
    "            self._modules[k] = v\n",
    "            super().__setattr__(k,v) #using python object's __setattr__\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self._modules}'\n",
    "\n",
    "    def parameters(self):\n",
    "        for l in self._modules.values():\n",
    "            for p in l.parameters():\n",
    "                yield p\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plain example\n",
    "class DumberModel(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.layer2 = nn.Linear(num_hidden, num_outputs)\n",
    "    def __call__(self, x):\n",
    "        return self.layer2(F.relu(self.layer1(x)))\n",
    "\n",
    "class DumbModel(nn.Module):\n",
    "    def __init__(self,layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for i, l in enumerate(self.layers): #add each layer as a Pytorch Module\n",
    "            self.add_module(f'layer_{i}', l) #name_index and the layer itself\n",
    "    def __call__(self, x):\n",
    "        return self.layer2(F.relu(self.layer1(x)))\n",
    "\n",
    "    \n",
    "#sequential model example\n",
    "class SequentialModel(nn.Module): #implementation of nn.Sequential\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range((n-1)//batch_size +1):\n",
    "            start_index = i * batch_size\n",
    "            x_mb = x_train[start_index:start_index + batch_size]\n",
    "            y_mb = y_train[start_index:start_index + batch_size]\n",
    "            predictions = model(x_mb)\n",
    "            loss = loss_function(predictions, y_mb)\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for parameter in model.parameters():\n",
    "                    parameter -= parameter.grad * learning_rate\n",
    "                model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Using the Sequential Layer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequentialModel(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_categories = 10\n",
    "layers = [nn.Linear(m, number_hid), nn.ReLU(), nn.Linear(number_hid, num_categories)]\n",
    "model = SequentialModel(layers)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2104, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function = F.cross_entropy\n",
    "batch_size = 64\n",
    "learning_rate = 0.5\n",
    "num_epochs = 1\n",
    "fit()\n",
    "loss_function(model(x_mini_batch), y_mini_batch), accuracy(model(x_mini_batch), y_mini_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pytorch's nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1953, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(m, number_hid), nn.ReLU(), nn.Linear(number_hid, num_categories))\n",
    "fit()\n",
    "loss_function(model(x_mini_batch), y_mini_batch), accuracy(model(x_mini_batch), y_mini_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Implementing torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, parameters_in, learning_rate_in = 0.5):\n",
    "        self.parameters = list(parameters_in)\n",
    "        self.lr = learning_rate_in\n",
    "        \n",
    "    def step(self): #executes one step of backward pass\n",
    "        with torch.no_grad():\n",
    "            for p in self.parameters:\n",
    "                p -= p.grad * self.lr\n",
    "    \n",
    "    def zero_gradients(self):\n",
    "        #prevents optimization of all of model's parameters\n",
    "        for p in self.parameters: #optimize the parameters passed to the parameters\n",
    "            p.grad.data.zero_()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m, number_hid), nn.ReLU(), nn.Linear(number_hid, num_categories))\n",
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding optimizer to the training loop\n",
    "def fit():\n",
    "    for i in range(epochs):\n",
    "        for i in range ((n-1)//batch_size +1):\n",
    "            start_i = i*batch_size\n",
    "            end_i = start_i+batch_size\n",
    "            x_mini_batch = x_train[start_i:end_i]\n",
    "            y_mini_batch = y_train[start_i:end_i]\n",
    "            predictions = model(x_mini_batch)\n",
    "            loss = loss_function(predictions, y_mini_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.3037, grad_fn=<NllLossBackward>), tensor(0.0781))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_function(model(x_mini_batch), y_mini_batch), accuracy(model(x_mini_batch), y_mini_batch)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mSource:\u001b[0m   \n",
       "    \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"\"\"Performs a single optimization step.\n",
       "\n",
       "        Arguments:\n",
       "            closure (callable, optional): A closure that reevaluates the model\n",
       "                and returns the loss.\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mweight_decay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mmomentum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'momentum'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mdampening\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dampening'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mnesterov\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nesterov'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0md_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0md_p\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mparam_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mif\u001b[0m \u001b[1;34m'momentum_buffer'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'momentum_buffer'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'momentum_buffer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0md_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0md_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\carson\\anaconda3\\envs\\py37\\lib\\site-packages\\torch\\optim\\sgd.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optim.SGD.step??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(m, number_hid), nn.ReLU(), nn.Linear(number_hid, num_categories))\n",
    "    return model, optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3175, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, optimizer = get_model()\n",
    "loss_function(model(x_mini_batch), y_mini_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Handling Data with Torch.utils.data.Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, x_ds, y_ds):\n",
    "        self.x_dataset = x_ds\n",
    "        self.y_dataset = y_ds\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_dataset)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x_dataset[i],self.y_dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing it out\n",
    "train_dataset, valid_dataset = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "assert len(train_dataset) == len(x_train) and len(valid_dataset) == len(x_valid)\n",
    "x_mb,y_mb = train_dataset[:5]\n",
    "assert x_mb.shape == (5,28*28) and y_mb.shape == (5, )\n",
    "x_mb, y_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = get_model()\n",
    "# def fit():\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range((n-1)//batch_size + 1):\n",
    "        xb,yb = train_dataset[i*batch_size : i*batch_size+batch_size]\n",
    "        pred = model(xb)\n",
    "        loss = loss_function(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "# fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0957, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_function(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():#making calls to datasets more fluent\n",
    "    def __init__(self, dataset, batchsize):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.dataset), self.batch_size):\n",
    "            yield self.dataset[i:i+self.batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dataset, batch_size)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb = next(iter(valid_dl))\n",
    "assert xb.shape==(batch_size,28*28)\n",
    "assert yb.shape==(batch_size,)\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0472, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the freshest model, optimizer, and training loop\n",
    "model, opt = get_model()\n",
    "def fit():\n",
    "    for epoch in range(num_epochs): #go through each epoch\n",
    "        for x_mb,y_mb in train_dl: #go through each batch, in size of mini_batch\n",
    "            predictions = model(x_mb) #calculate predictions on minibatch\n",
    "            loss = loss_function(predictions, y_mb) #calculate the loss\n",
    "            loss.backward() #calculate the gradients\n",
    "            opt.step() #apply the gradients to our weights, w/learning rate\n",
    "            opt.zero_grad() #zero out our weight's gradients\n",
    "            \n",
    "fit()\n",
    "loss, accuracy = loss_function(model(x_mb), y_mb), accuracy(model(x_mb), y_mb)\n",
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refactoring Dataloader to include randomness\n",
    "class Sampler():\n",
    "    def __init__(self, dataset, batchsize, shuffle = False):\n",
    "        self.length = len(dataset)\n",
    "        self.batch_size = batchsize\n",
    "        self.shuffle= shuffle\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.indexs = torch.randperm(self.length) if self.shuffle else torch.arrange(self.length)\n",
    "        for i in range(0, self.length, self.batch_size):\n",
    "            yield self.indexs[i:i+self.batch_size]\n",
    "            \n",
    "def collate(b):\n",
    "    xs, ys = zip(*b)\n",
    "    return torch.stack(xs), torch.stack(ys) #combines tensors on a new axis\n",
    "            \n",
    "class Dataloader():\n",
    "    def __init__(self, dataset, sampler, collate_fn = collate):\n",
    "        self.dataset = dataset\n",
    "        self.sampler = sample\n",
    "        self.collate_function = collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for sample in self.sampler:\n",
    "            yield self.collate_function([self.ds[i] for i in sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5), tensor(3))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#setup the samplers\n",
    "train_samp = Sampler(train_dataset, batch_size, shuffle=True)\n",
    "valid_samp = Sampler(valid_dataset, batch_size, shuffle=False)\n",
    "#setup the Dataloaders\n",
    "train_dl = DataLoader(train_dataset, train_samp)\n",
    "valid_dl = DataLoader(valid_dataset, train_samp)\n",
    "\n",
    "#show off some data\n",
    "fig, ax = plt.subplots(1,2)\n",
    "x_mb,y_mb = next(iter(valid_dl))\n",
    "x_mb1,y_mb1 = next(iter(train_dl))\n",
    "ax[0].imshow(x_mb1[0].view(28,28));\n",
    "ax[1].imshow(x_mb[0].view(28,28));\n",
    "y_mb1[0], y_mb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1416, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "fit()\n",
    "\n",
    "loss = loss_function(model(x_mb), y_mb)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pytorch Dataloader and Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1305, grad_fn=<NllLossBackward>) tensor(0.9688)\n",
      "tensor(0.1126, grad_fn=<NllLossBackward>) tensor(0.9531)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean() #redefined here, somehow tensor object was trying to be called\n",
    "train_dl = torch.utils.data.DataLoader(train_dataset, batch_size, sampler=RandomSampler(train_dataset), collate_fn=collate)\n",
    "valid_dl = torch.utils.data.DataLoader(valid_dataset, batch_size, sampler=SequentialSampler(valid_dataset), collate_fn=collate)\n",
    "\n",
    "model,opt = get_model()\n",
    "fit()\n",
    "print(loss_function(model(xb), yb), accuracy(model(xb), yb))\n",
    "\n",
    "#using Pytorch Defaults\n",
    "train_dl = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, drop_last=True)\n",
    "valid_dl = torch.utils.data.DataLoader(valid_dataset, batch_size, shuffle=False)\n",
    "\n",
    "model,opt = get_model()\n",
    "fit()\n",
    "print(loss_function(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Validating our  Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of layers that rely on the Training flag\n",
    "- nn.BatchNorm2d - 2-dimensional Batch Normalization\n",
    "- nn.Dropout - DropOut Module Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redefining fit to use Pytorch's Internal Attributes\n",
    "def fit(epoch, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for e in range(epoch):\n",
    "        model.train() #turn training flag to True\n",
    "        for x_mb, y_mb in train_dl:\n",
    "            predictions = model(x_mb)\n",
    "            loss = loss_func(predictions, y_mb)\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        model.eval() #turn training flag to False\n",
    "        with torch.no_grad():\n",
    "            total_loss, total_accuracy = 0.,0. #only works with equal batchsizes, need to implement weighted combinations\n",
    "            for x_mb, y_mb in valid_dl:\n",
    "                predictions = model(x_mb)\n",
    "                total_loss += loss_func(predictions, y_mb)\n",
    "                total_accuracy += accuracy(predictions, y_mb)\n",
    "        nv = len(valid_dl)\n",
    "        print(e, total_loss/nv, total_accuracy/nv)\n",
    "    return total_loss/nv, total_accuracy/nv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a simple function to get the dataloaders needed for training, note validation does not need to store the gradients so we can grab more\n",
    "def get_dataloaders(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (torch.utils.data.DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            torch.utils.data.DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "training in 3 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.3211) tensor(0.8911)\n",
      "1 tensor(0.1336) tensor(0.9595)\n",
      "2 tensor(0.1124) tensor(0.9665)\n",
      "3 tensor(0.1235) tensor(0.9638)\n",
      "4 tensor(0.1012) tensor(0.9725)\n"
     ]
    }
   ],
   "source": [
    "train_dl,valid_dl = get_dataloaders(train_dataset, valid_dataset, 64)\n",
    "model,opt = get_model()\n",
    "loss,acc = fit(5, model, loss_function, opt, train_dl, valid_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
