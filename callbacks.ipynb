{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.core.debugger import set_trace\n",
    "from fastai import datasets\n",
    "import pickle, gzip, math, torch, matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import tensor, nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
    "import torch.nn.functional as F\n",
    "# from data_utilities import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, x_ds, y_ds):\n",
    "        self.x_dataset = x_ds\n",
    "        self.y_dataset = y_ds\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_dataset)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x_dataset[i],self.y_dataset[i]\n",
    "\n",
    "class Callback():\n",
    "    def begin_fit(self, model, optimizer, loss_func, train_data, valid_data):\n",
    "        self.model = model\n",
    "        self.opt = optimizer\n",
    "        self.loss_function = loss_func\n",
    "        self.train_dl = train_data\n",
    "        self.valid_dl = valid_data\n",
    "        return True\n",
    "    \n",
    "    def after_fit(self):\n",
    "        return True\n",
    "    \n",
    "    def begin_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "        return True\n",
    "    \n",
    "    def begin_validate(self):\n",
    "        return True\n",
    "    \n",
    "    def after_epoch(self):\n",
    "        return True\n",
    "    \n",
    "    def begin_batch(self, x_batch, y_batch):\n",
    "        self.x_mini_batch = x_batch\n",
    "        self.y_mini_batch = y_batch\n",
    "        return True\n",
    "\n",
    "    def after_loss(self, loss):\n",
    "        self.loss = loss\n",
    "        return True\n",
    "    \n",
    "    def after_backward(self):\n",
    "        return True\n",
    "    def after_step(self):\n",
    "        return True\n",
    "    \n",
    "class CallbackHandler():\n",
    "    def __init__(self,cbs=None):\n",
    "        self.cbs = cbs if cbs else []\n",
    "\n",
    "    def begin_fit(self, model,optimizer, loss_func, train_data, valid_data):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_func\n",
    "        self.train_dl = train_data\n",
    "        self.valid_dl = valid_data\n",
    "        self.stop = False\n",
    "        self.in_train = True\n",
    "        res = True\n",
    "        for cb in self.cbs:\n",
    "                res = res and cb.begin_fit(model,optimizer, loss_func, train_dl, valid_dl)\n",
    "        return res\n",
    "\n",
    "    def after_fit(self):\n",
    "        res = not self.in_train\n",
    "        for cb in self.cbs: res = res and cb.after_fit()\n",
    "        return res\n",
    "    \n",
    "    def begin_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        self.in_train=True\n",
    "        res = True\n",
    "        for cb in self.cbs: res = res and cb.begin_epoch(epoch)\n",
    "        return res\n",
    "\n",
    "    def begin_validate(self):\n",
    "        self.model.eval()\n",
    "        self.in_train=False\n",
    "        res = True\n",
    "        for cb in self.cbs: res = res and cb.begin_validate()\n",
    "        return res\n",
    "\n",
    "    def after_epoch(self):\n",
    "        res = True\n",
    "        for cb in self.cbs: res = res and cb.after_epoch()\n",
    "        return res\n",
    "    \n",
    "    def begin_batch(self, xb, yb):\n",
    "        res = True\n",
    "        for cb in self.cbs: res = res and cb.begin_batch(xb, yb)\n",
    "        return res\n",
    "\n",
    "    def after_loss(self, loss):\n",
    "        res = self.in_train\n",
    "        for cb in self.cbs: res = res and cb.after_loss(loss)\n",
    "        return res\n",
    "\n",
    "    def after_backward(self):\n",
    "        res = True\n",
    "        for cb in self.cbs: res = res and cb.after_backward()\n",
    "        return res\n",
    "\n",
    "    def after_step(self):\n",
    "        res = True\n",
    "        for cb in self.cbs: \n",
    "            res = res and cb.after_step()\n",
    "            self.stop = cb.stop\n",
    "            if self.stop is False:\n",
    "                break\n",
    "        return res\n",
    "    \n",
    "    def do_stop(self): #signalled by a call back\n",
    "        try:     return self.stop\n",
    "        finally: self.stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redefining fit()\n",
    "def one_batch(x_minib, y_minib, callbacks):\n",
    "    if not callbacks.begin_batch(x_minib,y_minib):\n",
    "        return\n",
    "    loss = callbacks.loss_function(callbacks.model(x_minib), y_minib)\n",
    "    if not callbacks.after_loss(loss):\n",
    "        return\n",
    "    loss.backward()\n",
    "    if callbacks.after_backward(): \n",
    "        callbacks.optimizer.step()\n",
    "    if callbacks.after_step():\n",
    "        callbacks.optimizer.zero_grad()\n",
    "    \n",
    "def all_batches(dataloader, callbacks):\n",
    "    for x_minib, y_minib in dataloader:\n",
    "        one_batch(x_minib, y_minib, callbacks)\n",
    "        if callbacks.do_stop():\n",
    "            return\n",
    "    \n",
    "def fit(num_epochs, model, optimizer, loss_func, train_dataloader, valid_dataloader, callbacks):\n",
    "    if not callbacks.begin_fit(model, optimizer, loss_func, train_dataloader, valid_dataloader):\n",
    "        return\n",
    "    for epoch in range(num_epochs):\n",
    "        if not callbacks.begin_epoch(epoch):\n",
    "            continue\n",
    "        all_batches(train_dataloader, callbacks)\n",
    "        \n",
    "        if callbacks.begin_validate():\n",
    "            with torch.no_grad():\n",
    "                all_batches(valid_dataloader, callbacks)\n",
    "        if callbacks.do_stop() or not callbacks.after_epoch():\n",
    "            break\n",
    "        callbacks.after_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCallback(Callback):\n",
    "    def begin_fit(self,model, opt, loss_func, train_dl, valid_dl):\n",
    "        super().begin_fit(model, opt, loss_func, train_dl, valid_dl)\n",
    "        self.n_iters = 0\n",
    "        return True\n",
    "        \n",
    "    def after_step(self):\n",
    "        self.n_iters += 1\n",
    "        print(self.n_iters)\n",
    "        if self.n_iters>=10: \n",
    "            self.stop = True\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create simple 3 layer example model\n",
    "\n",
    "def get_model(training_data, lr=0.5, nh=50):\n",
    "    m = training_data.x_dataset.shape[1]\n",
    "    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,categories))\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'\n",
    "def get_data():\n",
    "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return map(tensor, (x_train,y_train,x_valid,y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_hidden = 50\n",
    "batch_size = 64\n",
    "loss_func = F.cross_entropy\n",
    "x_train,y_train,x_valid,y_valid = get_data()\n",
    "\n",
    "#setup data\n",
    "train_ds = Dataset(x_train, y_train)\n",
    "valid_ds = Dataset(x_valid, y_valid)\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, drop_last=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size, shuffle=False)\n",
    "categories = y_train.max().item()+1\n",
    "model, optimizer = get_model(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "fit(1, model, optimizer, loss_func, train_dl, valid_dl, callbacks=CallbackHandler([TestCallback()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
